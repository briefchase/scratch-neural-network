{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup details."
      ],
      "metadata": {
        "id": "bnJOeO3UB4n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dLW5D70LB2f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data."
      ],
      "metadata": {
        "id": "YS33gaCtehJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compile data \"\"\"\n",
        "\n",
        "def compile_dataset():\n",
        "  X = [] # Initialize the dataset itself\n",
        "  Y = [] # The corresponding label (Y) for each entry of X\n",
        "\n",
        "  # loop through all possible 4-bit combinations\n",
        "  for i in range(16):\n",
        "    # Generate new value\n",
        "    x = tuple(format(i, '04b'))\n",
        "    # count the number of ones\n",
        "    num_ones = x.count('1')\n",
        "    # Add to X\n",
        "    X.append(list(x))\n",
        "    # set the value of the corresponding key to 1 if there are an odd number of ones, 0 otherwise\n",
        "    if num_ones % 2 == 1: Y.append(1)\n",
        "    else: Y.append(0)\n",
        "  return np.array(X, dtype=np.float64), np.array(Y, dtype=np.float64)\n"
      ],
      "metadata": {
        "id": "H8g6fvoHelE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the network model."
      ],
      "metadata": {
        "id": "U3gbg2q-AsxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec-RRuKx59vm"
      },
      "outputs": [],
      "source": [
        "\"\"\" Options \"\"\"\n",
        "\n",
        "# Literals that define the network architecture\n",
        "INPUTS = 1 # The ammount of inputs in X\n",
        "LAYERS = [4, 1] # An array containing the number of neurons in each layer\n",
        "\n",
        "\"\"\" Build and Initialize the model \"\"\"\n",
        "\n",
        "# Build and return the network of weights and biases\n",
        "def build_model():\n",
        "  # Build the model layer by layer\n",
        "  weights = []\n",
        "  for l in range(len(LAYERS)):\n",
        "    inputs = INPUTS if l == 0 else LAYERS[l-1] # Define the number of inputs for each layer\n",
        "    neurons = LAYERS[l] # Define the number of neurons\n",
        "    layer = np.asarray([[None for _ in range(inputs + 1)] for _ in range(neurons)]) # Create an array containing the weights for each neuron in the layer including bias\n",
        "    weights.append(layer) # Add the layer to the weights\n",
        "  return weights\n",
        "\n",
        "# Initialize the weights of the model\n",
        "def init_params(model):\n",
        "  # Initialize the weights randomly\n",
        "  for layer in range(len(model)):\n",
        "    for neuron in range(len(model[layer])):\n",
        "      for weight in range(len(model[layer][neuron])):\n",
        "        model[layer][neuron, weight] = np.random.rand()*2-1\n",
        "\n",
        "\"\"\" Functions relevant to the model usage \"\"\"\n",
        "\n",
        "# The activation function to be used for calculating the output of a layer\n",
        "def activation(z):\n",
        "  return 1 / ( 1 + np.exp(-z.astype(np.float64))) # Return the result of a sigmoid function\n",
        "\n",
        "# Calculate the outputs/prediction (yhat) using a forward pass through the network\n",
        "def forward_pass(model, X):\n",
        "  cache = [] #Initialize the empty cache\n",
        "  for layer in range(len(model)): # Iterate through every layer\n",
        "    inputs = np.array(X, dtype=np.float64) if layer == 0 else cache[layer-1][1]\n",
        "    weight_matrix = model[layer] # Grab all the weights & biases from the layer\n",
        "    layer_bias = weight_matrix[:, 0] # Extract bias from the layer\n",
        "    layer_weights = weight_matrix[:, 1:]  # Extract weights from the layer\n",
        "    hypothesis =  np.dot(inputs, layer_weights.T) + layer_bias # Compute the linear hypothesyses\n",
        "    input_cache = (inputs, weight_matrix) # Box up and store variables use in calculation\n",
        "    activation_cache = activation(hypothesis) # Compute activation_cache\n",
        "    cache.append((input_cache, activation_cache))\n",
        "  return cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define how the model will learn."
      ],
      "metadata": {
        "id": "GEhX39ktA1Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Functions used in learning \"\"\"\n",
        "\n",
        "# Binary cross entropy loss function\n",
        "def error(y_true, y_pred, eps=1e-15):\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)  # Clip y_pred to avoid 0 or 1 values\n",
        "    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# Derivitive of binary cross entropy loss\n",
        "def error_derivitive(y_true, y_pred, eps=1e-15):\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)  # Clip y_pred to avoid 0 or 1 values\n",
        "    d_loss_d_y_pred = - (y_true / y_pred - (1 - y_true) / (1 - y_pred + eps))\n",
        "    return d_loss_d_y_pred\n",
        "\n",
        "# Perform back propigation in order to learn gradients\n",
        "def back_prop(labels, caches):\n",
        "  # Prep neccesary variables\n",
        "  num_layers = len(caches)\n",
        "  num_examples = len(labels)\n",
        "  predictions = caches[len(params)-1][1]\n",
        "  Y = labels.reshape(predictions.shape)\n",
        "  # Build 'gradients'\n",
        "  gradients = [None for _ in range(num_layers)]\n",
        "  # Calculate derivitives w.r.t. prediction\n",
        "  err_derivitives = error_derivitive(Y, predictions) #-(Y / predictions - (1 - Y) / (1 - predictions))\n",
        "  # Calculate gradient for first layer\n",
        "  current_cache = caches[num_layers-1]\n",
        "  err_derivitives, gradients[num_layers-1] = calc_gradients(err_derivitives, current_cache)\n",
        "  # Calculate gradient for remaining layers\n",
        "  for layer in reversed(range(num_layers-1)):\n",
        "    current_cache = caches[layer]\n",
        "    err_derivitives, gradients[layer] = calc_gradients(err_derivitives, current_cache)\n",
        "  return gradients\n",
        "  \n",
        "\n",
        "def calc_gradients(e, cache):\n",
        "  # Unbox neccesary variables\n",
        "  input_cache, activation_cache = cache\n",
        "  input, weight_matrix = input_cache\n",
        "  layer_bias = weight_matrix[:, 0] # Extract bias from the layer\n",
        "  layer_weights = weight_matrix[:, 1:]  # Extract weights from the layer\n",
        "  m = input.shape[1]\n",
        "  # Calculate the derivitives of the sigmoid function\n",
        "  delta = e*activation(activation_cache) * (1-activation(activation_cache))\n",
        "  # Calculate gradients\n",
        "  de = np.sum(np.dot(delta, layer_weights), axis=1)[np.newaxis].T\n",
        "  dw = (1/m)* np.sum(delta * activation_cache, axis=0)\n",
        "  db = (1/m)* np.sum(delta, axis=0, keepdims=True)\n",
        "  return de, (dw, db)\n",
        "\n",
        "  # Calculate the derivitives of the sigmoid function\n",
        "  #delta = e*(activation(activation_cache) * (1-activation(activation_cache)))\n",
        "  # Calculate gradients\n",
        "  #de = np.dot(delta, layer_weights)\n",
        "  #dw = (1/m)*np.dot(delta.T, input)\n",
        "  #db = (1/m)*np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "def update_weights(model, gradients, learning_rate):\n",
        "  for l in range(len(model)):\n",
        "    model[l][:, 1:] = model[l][:, 1:] - learning_rate*gradients[l][0] # Update Weights\n",
        "    model[l][:, 0] = model[l][:, 0] - learning_rate*gradients[l][1] # Update Bias\n"
      ],
      "metadata": {
        "id": "EDCHRRv0A0jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "RzEK_NnBA61t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Options \"\"\"\n",
        "\n",
        "EPOCHS = 5000\n",
        "LEARNING_RATE = .075\n",
        "\n",
        "\"\"\" Prepare the data \"\"\"\n",
        "\n",
        "# Get the dataset\n",
        "X, Y = compile_dataset()\n",
        "\n",
        "# Permute and sample the dataset\n",
        "#np.random.shuffle(X)\n",
        "\n",
        "# Create the model\n",
        "params = build_model()\n",
        "init_params(params)\n",
        "\n",
        "\"\"\" Train the model... \"\"\"\n",
        "\n",
        "for epoch in range(EPOCHS): # Iterations through the data\n",
        "  caches = forward_pass(params, X) # Results of forward pass\n",
        "  yhat = caches[len(params)-1][1].flatten() # Predictions\n",
        "  grad = back_prop(Y, caches) # Gradients\n",
        "  update_weights(params, grad, LEARNING_RATE) # Update the weights with the calculated gradients\n",
        "\n",
        "\"\"\" Evaluate final accuracy... \"\"\"\n",
        "\n",
        "print(\"Training set accuracy = \" + str(np.sum(np.where(yhat < .5, 1, 0))/len(X)))\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "sA4YmOUvBDOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5842fbed-807e-4533-d184-c0c3b58f432f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set accuracy = 0.5\n"
          ]
        }
      ]
    }
  ]
}